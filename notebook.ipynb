{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchaudio\n",
    "import IPython.display as ipd\n",
    "from knn_vc.wav_convert import convert_flac_to_wav\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN-VC Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "download_dir = \"~/media/Data/CambAI/TakeHome/CambAI_Takehome/knn-vc-downloads\"\n",
    "torch.hub.set_dir(download_dir)\n",
    "knn_vc = torch.hub.load('bshall/knn-vc', 'knn_vc', prematched=True, trust_repo=True, pretrained=True, device=\"cuda:0\")\n",
    "# torch.save(knn_vc.state_dict(), \"./knn_vc_state_dict\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "src_wav_path = \"./datasets/debug_set/srcs/4507-16021-0024.flac\" # 16kHz file to be voice-converted\n",
    "ref_wav_paths = [\"./datasets/debug_set/targets/61-70968-0000.flac\", \"./datasets/debug_set/targets/61-70968-0001.flac\", \"./datasets/debug_set/targets/61-70968-0002.flac\", \n",
    "                 \"./datasets/debug_set/targets/61-70968-0003.flac\"] # list of 16kHz files that serve as the target, the src_wav_path will be converted to this speaker\n",
    "\n",
    "\n",
    "query_seq = knn_vc.get_features(src_wav_path) # Returns features of `path` waveform as a tensor of shape (seq_len, dim) --> data preprocessing\n",
    "matching_set = knn_vc.get_matching_set(ref_wav_paths) # Get matching features to be used by wavlm --> Data preprocessing\n",
    "\n",
    "out_wav = knn_vc.match(query_seq, matching_set, topk=4) # Using matcher to perform VC, basically VC.forward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(out_wav.numpy(), rate=16000)\n",
    "torchaudio.save('knnvc1_out.wav', out_wav[None], 16000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torchserve Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ts.torch_handler.base_handler import BaseHandler\n",
    "import torch\n",
    "\n",
    "class KNN_VC_Handler(BaseHandler):\n",
    "    \"\"\"\n",
    "    Refer to https://pytorch.org/serve/custom_service.html\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self._context = None\n",
    "        self.initialized = None\n",
    "        self.model = None\n",
    "        self.device = None\n",
    "\n",
    "\n",
    "    def initialize(self,context):\n",
    "        \"\"\"\n",
    "        Initialize model and resources here\n",
    "        \"\"\"\n",
    "        # From the exapmle:\n",
    "        self.manifest = context.manifest\n",
    "        properties = context.system_properties\n",
    "        self.device = torch.device(\"cuda:\" + str(properties.get(\"gpu_id\")) if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Download model\n",
    "        self.model = torch.hub.load('bshall/knn-vc', 'knn_vc', prematched=True, trust_repo=True, pretrained=True, device=self.device)\n",
    "        print(f\"==================================================================================\")\n",
    "        print(f\"==================================================================================\")\n",
    "        print(f\"Self.model: {self.model}\")\n",
    "        print(f\"==================================================================================\")\n",
    "        print(f\"==================================================================================\")\n",
    "        self.initialized = True\n",
    "\n",
    "    def preprocess(self, data):\n",
    "        src_wav_path = data[\"source_path\"]\n",
    "        ref_wav_paths = data[\"target_paths\"]\n",
    "\n",
    "        query_seq = self.model.get_features(src_wav_path) # Returns features of `path` waveform as a tensor of shape (seq_len, dim) --> data preprocessing\n",
    "        matching_set = self.model.get_matching_set(ref_wav_paths) # Get matching features to be used by wavlm --> Data preprocessing\n",
    "\n",
    "        return query_seq, matching_set\n",
    "\n",
    "    def handle(self, data:dict, context):\n",
    "        \"\"\"\n",
    "        Invoke by TorchServe for prediction request.\n",
    "        Do pre-processing of data, prediction using model and postprocessing of prediciton output\n",
    "            data:   Input data for prediction\n",
    "            context:    Initial context contains model server system properties.\n",
    "            return: prediction output\n",
    "        \"\"\"\n",
    "\n",
    "        # First pre-process the data. Accept data as a dict, and get the stuff\n",
    "        query_seq, matching_set = self.preprocess(data)\n",
    "        \n",
    "        out_wav = self.model.match(query_seq, matching_set, topk=4)\n",
    "\n",
    "        return out_wav\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate MAR File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dummy Handler and Model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "\n",
    "# command = [\n",
    "#     'torch-model-archiver',\n",
    "#     '--model-name', 'dummy_model',\n",
    "#     '--version', '1.0',\n",
    "#     '--model-file', 'dummy_model.py',\n",
    "#     '--serialized-file', 'dummymodel_state',\n",
    "#     '--handler', 'dummy_handler.py',\n",
    "#     '--export-path', 'deployment/model_store',\n",
    "#     '-f'\n",
    "# ]\n",
    "\n",
    "command = [\n",
    "    'torch-model-archiver',\n",
    "    '--model-name', 'knn_vc',\n",
    "    '--version', '1.0',\n",
    "    '--model-file', 'knn_vc/matcher.py',\n",
    "    '--serialized-file', 'knn_vc_state_dict',\n",
    "    '--handler', 'knn_vc_handler.py',\n",
    "    '--export-path', 'deployment/model_store',\n",
    "    '-f'\n",
    "]\n",
    "# command = [\"ls\"]\n",
    "\n",
    "result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"Command executed successfully.\")\n",
    "    print(\"Output:\")\n",
    "    print(result.stdout)\n",
    "else:\n",
    "    print(\"Command failed with the following error:\")\n",
    "    print(result.stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- torch-model-archiver --model-name knn_vc --version 1.0 --model-file knn_vc/matcher.py --serialized-file ./knn_vc_state_dict --handler knn_vc/knn_vc_handler.py -->\n",
    "\n",
    "torch-model-archiver --model-name knn_vc --version 1.0 --model-file knn_vc/dummy_model.py --serialized-file dummymodel_state --handler knn_vc/knn_vc_handler.py --export-path deployment/model_store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torchserve --start --ncs --model-store deployment/model_store --models knn_vc=knn_vc_test.mar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from knn_vc import hubconf as hc\n",
    "\n",
    "# model = knn_vc(pretrained=True, progress=True, prematched=True, device='cuda')\n",
    "\n",
    "# model = DummyModel()\n",
    "\n",
    "# print(model.get_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.state_dict())\n",
    "\n",
    "torch.save(model.state_dict(), \"dummymodel_state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSON stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import torchaudio\n",
    "import requests\n",
    "from generate_input import generate_json_files_for_inference,convert_json_paths_to_json_lists\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def send_audio_to_server(server_url, json_dict, id):\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    response = requests.post(server_url, data=json_dict)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        # result = response.json()\n",
    "        # Process the inference result\n",
    "        print(\"======================================\")\n",
    "        print(f\"Inference successful for request number {id}\")\n",
    "        print(\"======================================\")\n",
    "        json_response = response.json()\n",
    "        torchaudio.save(f\"output_files/output_{id}.wav\", torch.tensor(json_response).unsqueeze(0), 16000)\n",
    "\n",
    "    else:\n",
    "        # Handle errors or log them\n",
    "        print(f\"Request failed with status code: {response.status_code}\")\n",
    "\n",
    "\n",
    "def generate_noise():\n",
    "    sr = 16000\n",
    "    min_audio_length = 3*sr\n",
    "    max_audio_length = 25*sr\n",
    "    mean = 0\n",
    "    std = 1\n",
    "\n",
    "\n",
    "    max_num_targets = 20\n",
    "    min_num_targets = 1\n",
    "\n",
    "    num_targets = np.random.randint(min_num_targets, max_num_targets)\n",
    "\n",
    "    num_src_samples = np.random.randint(min_audio_length, max_audio_length)\n",
    "\n",
    "    src = list(np.random.normal(mean, std, size=num_src_samples))\n",
    "\n",
    "    targets = []\n",
    "    for i in range(num_targets):\n",
    "        ns = np.random.randint(min_audio_length, max_audio_length)\n",
    "        trgt = list(np.random.normal(mean, std, size=ns))\n",
    "        targets.append(trgt)\n",
    "        \n",
    "    data = {\n",
    "            \"source_audio\": src,\n",
    "            \"target_audios\": targets\n",
    "            }\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "for i in range (4):\n",
    "    st = time.time()\n",
    "    paths = generate_json_files_for_inference(\"./datasets/LibriSpeech/test-clean\")\n",
    "    temp = convert_json_paths_to_json_lists(paths)\n",
    "    print(f\"Generating files from paths took {time.time()-st} seconds\")\n",
    "    \n",
    "    st = time.time()\n",
    "    json_dict = generate_noise()\n",
    "    print(f\"Generating noise took {time.time()-st} seconds\")\n",
    "    # with open(json_file, \"r\") as json_file:\n",
    "    #     json_dict = json.loads(json_file.read()) #loads the config yaml file as a dict\n",
    "\n",
    "    print(type(json_dict[\"source_audio\"]))\n",
    "    server_url = f\"http://localhost:8080/predictions/knn_vc\"\n",
    "    # Sending to server via python script:\n",
    "    # let's manually convert to json\n",
    "    json_string = json.dumps(json_dict)\n",
    "    byte_array = json_string.encode('utf-8')\n",
    "    send_audio_to_server(server_url, byte_array, 0)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "# print(json_dict)\n",
    "\n",
    "# print(os.path.abspath(json_dict[\"source_path\"]))\n",
    "\n",
    "# audio = torchaudio.load(json_dict[\"source_path\"])\n",
    "\n",
    "# print(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def generate_noise():\n",
    "    sr = 16000\n",
    "    min_audio_length = 3*sr\n",
    "    max_audio_length = 25*sr\n",
    "    mean = 0\n",
    "    std = 1\n",
    "\n",
    "\n",
    "    max_num_targets = 20\n",
    "    min_num_targets = 1\n",
    "\n",
    "    num_targets = np.random.randint(min_num_targets, max_num_targets)\n",
    "\n",
    "    num_src_samples = np.random.randint(min_audio_length, max_audio_length)\n",
    "\n",
    "    src = np.random.normal(mean, std, size=num_src_samples)\n",
    "\n",
    "    targets = []\n",
    "    for i in range(num_targets):\n",
    "        ns = np.random.randint(min_audio_length, max_audio_length)\n",
    "        trgt = np.random.normal(mean, std, size=ns)\n",
    "        targets.append(trgt)\n",
    "        \n",
    "    data = {\n",
    "            \"source_audio\": src,\n",
    "            \"target_audios\": targets\n",
    "            }\n",
    "\n",
    "    return data\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# def generate_json_files_for_inference():\n",
    "dataset_path = \"./datasets/LibriSpeech/test-clean\"\n",
    "num_readers = len(next(os.walk(dataset_path))[1]) # number of folders, each is a unique reader\n",
    "\n",
    "# select a random reader\n",
    "directories = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n",
    "\n",
    "src_reader_id = np.random.randint(0,num_readers) #randomly select a reader id for the source\n",
    "src_reader_dir = directories[src_reader_id] # reader dir for the source utterances\n",
    "\n",
    "target_reader_id = np.random.randint(0,num_readers)\n",
    "\n",
    "while target_reader_id == src_reader_id:\n",
    "    target_reader_id = np.random.randint(0,num_readers) #so src and target aren't the same\n",
    "\n",
    "target_reader_dir = directories[target_reader_id] # reader dir for the target utterances\n",
    "\n",
    "# src_num_chapters_avail = len(next(os.walk(f\"{dataset_path}/{src_reader_dir}\"))[1]) #how many folders the src reader has to select from\n",
    "# target_num_chapters_avail = len(next(os.walk(f\"{dataset_path}/{target_reader_dir}\"))[1]) #how many folders the target reader has to select from\n",
    "\n",
    "src_chapter_dirs = [d for d in os.listdir(f\"{dataset_path}/{src_reader_dir}\") if os.path.isdir(os.path.join(f\"{dataset_path}/{src_reader_dir}\", d))] #the actual names of the chapter dirs for the chosen speaker\n",
    "target_chapter_dirs = [d for d in os.listdir(f\"{dataset_path}/{target_reader_dir}\") if os.path.isdir(os.path.join(f\"{dataset_path}/{target_reader_dir}\", d))] #the actual names of the chapter dirs for the chosen target\n",
    "\n",
    "src_chapter = random.choice(src_chapter_dirs)\n",
    "target_chapter = random.choice(target_chapter_dirs)\n",
    "\n",
    "src_path = f\"{dataset_path}/{src_reader_dir}/{src_chapter}\"\n",
    "target_path = f\"{dataset_path}/{target_reader_dir}/{target_chapter}\"\n",
    "\n",
    "# choosing a random .flac file for the src utterance\n",
    "src_files = [f for f in os.listdir(src_path) if os.path.isfile(os.path.join(src_path, f))]\n",
    "src_flac_files =[f for f in src_files if f.endswith('.flac')]\n",
    "src_file = random.choice(src_flac_files)\n",
    "\n",
    "# choosing a random number of .flac files for the target utterance\n",
    "target_files = [f for f in os.listdir(target_path) if os.path.isfile(os.path.join(target_path, f))]\n",
    "target_flac_files =[f for f in target_files if f.endswith('.flac')]\n",
    "num_files_to_select = 1\n",
    "# num_files_to_select = np.random.randint(1, len(target_flac_files)-1)\n",
    "print(num_files_to_select)\n",
    "print(len(target_flac_files))\n",
    "\n",
    "target_files = random.sample(target_flac_files,num_files_to_select)\n",
    "\n",
    "src_path_final = f\"{dataset_path}/{src_reader_dir}/{src_chapter}/{src_file}\"\n",
    "target_paths_final = [f\"{dataset_path}/{target_reader_dir}/{target_chapter}/{target_file}\" for target_file in target_files]\n",
    "\n",
    "data = {\n",
    "    \"source_path\": src_path_final,\n",
    "    \"target_paths\": target_paths_final,\n",
    "    \"working_dir\": \"/media/Data/CambAI/TakeHome/CambAI_TakeHome\"\n",
    "}\n",
    "\n",
    "print(data)\n",
    "\n",
    "# print(f\"Choosing {src_reader_dir}/{src_chapter}/{src_file} for src\")\n",
    "# print(f\"Choosing {num_files_to_select} files {target_reader_dir}/{target_chapter}/{target_files} for target\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "sudo docker run --gpus 1 --rm -it -p 127.0.0.1:8080:8080 -p 127.0.0.1:8081:8081 -p 127.0.0.1:8082:8082 --name knn_vc -v /home/dewet/Documents/Camb/deployment/model_store:/home/model-server/model-store -v /home/dewet/Documents/Camb/knn_vc:/home/model-server/knn_vc -v /home/dewet/Documents/Camb/utils:/home/model-server/utils -v /home/dewet/Documents/Camb/config.properties:/home/model-server/config.properties pytorch/torchserve:latest-gpu\n",
    "\n",
    "<!-- Custom docker image -->\n",
    "sudo docker run --rm -it -p 127.0.0.1:8080:8080 -p 127.0.0.1:8081:8081 -p 127.0.0.1:8082:8082 --name knn_vc -v /home/dewet/Documents/Camb/deployment/model_store:/home/model-server/model-store -v /home/dewet/Documents/Camb/knn_vc:/home/model-server/knn_vc -v /home/dewet/Documents/Camb/dewet_utils:/home/model-server/dewet_utils pytorch/custom_torchserve:latest\n",
    "\n",
    "\n",
    "sudo docker exec -it knn_vc /bin/bash\n",
    "\n",
    "torch-model-archiver --model-name knn_vc --version 1.0 --model-file /home/model-server/knn_vc/matcher.py --serialized-file /home/model-server/knn_vc/state_dicts/knn_vc_state_dict --export-path /home/model-server/model-store --handler /home/model-server/dewet_utils/knn_vc_handler.py -f\n",
    "\n",
    "\n",
    "<!-- Reload Hanlder -->\n",
    "torch-model-archiver --model-name knn_vc --handler /home/model-server/dewet_utils/knn_vc_handler.py --export-path /home/model-server/model-store -f -v 1.0\n",
    "\n",
    "<!-- Build custom dockerfile -->\n",
    "docker build -t pytorch/custom_torchserve:latest -f Dockerfile.txt .\n",
    "\n",
    "<!-- List dcker images -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/home/model-server/knn_vc/state_dicts/knn_vc_state_dict.pth\n",
    "\n",
    "<!-- Register Model -->\n",
    "curl -X POST \"localhost:8081/models?model_name=knn_vc&url=/home/model-server/model-store/knn_vc.mar&initial_workers=1\"\n",
    "\n",
    "<!-- Inference from CLI -->\n",
    "curl http://localhost:8080/predictions/knn_vc -T \"./input_data.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting .flac files to tensors and sending via JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from send_audio_to_server import generate_json_files_for_inference, convert_json_paths_to_json_lists\n",
    "import torch\n",
    "import torchaudio\n",
    "import json\n",
    "import sys\n",
    "import copy\n",
    "import time\n",
    "\n",
    "path = generate_json_files_for_inference(\"./datasets/LibriSpeech/test-clean\")\n",
    "# print(path)\n",
    "\n",
    "data = convert_json_paths_to_json_lists(path)\n",
    "\n",
    "\n",
    "# json_file_path = \"temp_dict.json\"\n",
    "\n",
    "# with open(json_file_path, \"w\") as json_file:\n",
    "#     json.dump(audio_dict, json_file, ensure_ascii=False)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# waveform, sample_rate = torchaudio.load(path[\"source_path\"], normalize=True)\n",
    "\n",
    "# tensor_list = waveform.tolist()\n",
    "\n",
    "# print(f\"tensor list: {len(tensor_list[0])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_string = json.dumps(data)\n",
    "\n",
    "byte_array = json_string.encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = byte_array.decode('utf-8')\n",
    "inputs = json.loads(inputs)\n",
    "\n",
    "print(len(inputs[\"source_audio\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsr = torch.ones(124)\n",
    "\n",
    "print(tsr.shape)\n",
    "\n",
    "lst = tsr.tolist()\n",
    "\n",
    "print(len(lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# j=0\n",
    "# # Monitor the responses directory for new files\n",
    "# while true; do\n",
    "#     inotifywait -e create /output_files/jsons  # Wait for a new file to be created\n",
    "#     latest_file=$(ls -t /output_files/jsons  | head -1)  # Get the latest response file\n",
    "#     if [[ -f \"/output_files/jsons/$latest_file\" ]]; then\n",
    "#     \tj++\n",
    "#         # Process the latest response file using a Python script\n",
    "#         python save_audio.py $j < \"responses/$latest_file\"\n",
    "#     fi\n",
    "# done\n",
    "\n",
    "\n",
    "j=0\n",
    "\n",
    "# Monitor the responses directory for new files\n",
    "while true; do\n",
    "    inotifywait -e create ./output_files/jsons  # Wait for a new file to be created\n",
    "    latest_file=$(ls -t ./output_files/jsons  | head -1)  # Get the latest response file\n",
    "    echo \"Latest file is $latest_file\"\n",
    "    if [[ -f \"./output_files/jsons/$latest_file\" ]]; then\n",
    "        j=$((j+1))  # Increment j\n",
    "        # Process the latest response file using a Python script\n",
    "        python3 save_audio.py $j < \"./output_files/jsons/$latest_file\"\n",
    "        \n",
    "        if [ \"$j\" -eq \"$num_runs\" ]; then\n",
    "            # Exit the loop when j reaches the number of runs\n",
    "            exit 1\n",
    "            \n",
    "        fi\n",
    "    fi\n",
    "done\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
