{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchaudio\n",
    "import IPython.display as ipd\n",
    "from knn_vc.wav_convert import convert_flac_to_wav\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN-VC Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "download_dir = \"~/media/Data/CambAI/TakeHome/CambAI_Takehome/knn-vc-downloads\"\n",
    "torch.hub.set_dir(download_dir)\n",
    "knn_vc = torch.hub.load('bshall/knn-vc', 'knn_vc', prematched=True, trust_repo=True, pretrained=True, device=\"cuda:0\")\n",
    "# torch.save(knn_vc.state_dict(), \"./knn_vc_state_dict\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "src_wav_path = \"./datasets/debug_set/srcs/4507-16021-0024.flac\" # 16kHz file to be voice-converted\n",
    "ref_wav_paths = [\"./datasets/debug_set/targets/61-70968-0000.flac\", \"./datasets/debug_set/targets/61-70968-0001.flac\", \"./datasets/debug_set/targets/61-70968-0002.flac\", \n",
    "                 \"./datasets/debug_set/targets/61-70968-0003.flac\"] # list of 16kHz files that serve as the target, the src_wav_path will be converted to this speaker\n",
    "\n",
    "\n",
    "query_seq = knn_vc.get_features(src_wav_path) # Returns features of `path` waveform as a tensor of shape (seq_len, dim) --> data preprocessing\n",
    "matching_set = knn_vc.get_matching_set(ref_wav_paths) # Get matching features to be used by wavlm --> Data preprocessing\n",
    "\n",
    "out_wav = knn_vc.match(query_seq, matching_set, topk=4) # Using matcher to perform VC, basically VC.forward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(out_wav.numpy(), rate=16000)\n",
    "torchaudio.save('knnvc1_out.wav', out_wav[None], 16000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torchserve Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ts.torch_handler.base_handler import BaseHandler\n",
    "import torch\n",
    "\n",
    "class KNN_VC_Handler(BaseHandler):\n",
    "    \"\"\"\n",
    "    Refer to https://pytorch.org/serve/custom_service.html\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self._context = None\n",
    "        self.initialized = None\n",
    "        self.model = None\n",
    "        self.device = None\n",
    "\n",
    "\n",
    "    def initialize(self,context):\n",
    "        \"\"\"\n",
    "        Initialize model and resources here\n",
    "        \"\"\"\n",
    "        # From the exapmle:\n",
    "        self.manifest = context.manifest\n",
    "        properties = context.system_properties\n",
    "        self.device = torch.device(\"cuda:\" + str(properties.get(\"gpu_id\")) if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Download model\n",
    "        self.model = torch.hub.load('bshall/knn-vc', 'knn_vc', prematched=True, trust_repo=True, pretrained=True, device=self.device)\n",
    "        print(f\"==================================================================================\")\n",
    "        print(f\"==================================================================================\")\n",
    "        print(f\"Self.model: {self.model}\")\n",
    "        print(f\"==================================================================================\")\n",
    "        print(f\"==================================================================================\")\n",
    "        self.initialized = True\n",
    "\n",
    "    def preprocess(self, data):\n",
    "        src_wav_path = data[\"source_path\"]\n",
    "        ref_wav_paths = data[\"target_paths\"]\n",
    "\n",
    "        query_seq = self.model.get_features(src_wav_path) # Returns features of `path` waveform as a tensor of shape (seq_len, dim) --> data preprocessing\n",
    "        matching_set = self.model.get_matching_set(ref_wav_paths) # Get matching features to be used by wavlm --> Data preprocessing\n",
    "\n",
    "        return query_seq, matching_set\n",
    "\n",
    "    def handle(self, data:dict, context):\n",
    "        \"\"\"\n",
    "        Invoke by TorchServe for prediction request.\n",
    "        Do pre-processing of data, prediction using model and postprocessing of prediciton output\n",
    "            data:   Input data for prediction\n",
    "            context:    Initial context contains model server system properties.\n",
    "            return: prediction output\n",
    "        \"\"\"\n",
    "\n",
    "        # First pre-process the data. Accept data as a dict, and get the stuff\n",
    "        query_seq, matching_set = self.preprocess(data)\n",
    "        \n",
    "        out_wav = self.model.match(query_seq, matching_set, topk=4)\n",
    "\n",
    "        return out_wav\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate MAR File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dummy Handler and Model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "\n",
    "# command = [\n",
    "#     'torch-model-archiver',\n",
    "#     '--model-name', 'dummy_model',\n",
    "#     '--version', '1.0',\n",
    "#     '--model-file', 'dummy_model.py',\n",
    "#     '--serialized-file', 'dummymodel_state',\n",
    "#     '--handler', 'dummy_handler.py',\n",
    "#     '--export-path', 'deployment/model_store',\n",
    "#     '-f'\n",
    "# ]\n",
    "\n",
    "command = [\n",
    "    'torch-model-archiver',\n",
    "    '--model-name', 'knn_vc',\n",
    "    '--version', '1.0',\n",
    "    '--model-file', 'knn_vc/matcher.py',\n",
    "    '--serialized-file', 'knn_vc_state_dict',\n",
    "    '--handler', 'knn_vc_handler.py',\n",
    "    '--export-path', 'deployment/model_store',\n",
    "    '-f'\n",
    "]\n",
    "# command = [\"ls\"]\n",
    "\n",
    "result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"Command executed successfully.\")\n",
    "    print(\"Output:\")\n",
    "    print(result.stdout)\n",
    "else:\n",
    "    print(\"Command failed with the following error:\")\n",
    "    print(result.stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- torch-model-archiver --model-name knn_vc --version 1.0 --model-file knn_vc/matcher.py --serialized-file ./knn_vc_state_dict --handler knn_vc/knn_vc_handler.py -->\n",
    "\n",
    "torch-model-archiver --model-name knn_vc --version 1.0 --model-file knn_vc/dummy_model.py --serialized-file dummymodel_state --handler knn_vc/knn_vc_handler.py --export-path deployment/model_store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torchserve --start --ncs --model-store deployment/model_store --models knn_vc=knn_vc_test.mar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wavlm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/media/Data/CambAI/TakeHome/CambAI_TakeHome/notebook.ipynb Cell 16\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/Data/CambAI/TakeHome/CambAI_TakeHome/notebook.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/media/Data/CambAI/TakeHome/CambAI_TakeHome/notebook.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mknn_vc\u001b[39;00m \u001b[39mimport\u001b[39;00m hubconf \u001b[39mas\u001b[39;00m hc\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/Data/CambAI/TakeHome/CambAI_TakeHome/notebook.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# model = knn_vc(pretrained=True, progress=True, prematched=True, device='cuda')\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/Data/CambAI/TakeHome/CambAI_TakeHome/notebook.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/media/Data/CambAI/TakeHome/CambAI_TakeHome/notebook.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# model = DummyModel()\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/Data/CambAI/TakeHome/CambAI_TakeHome/notebook.ipynb#X16sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/media/Data/CambAI/TakeHome/CambAI_TakeHome/notebook.ipynb#X16sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# print(model.get_features)\u001b[39;00m\n",
      "File \u001b[0;32m/media/Data/CambAI/TakeHome/CambAI_TakeHome/knn_vc/hubconf.py:12\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpathlib\u001b[39;00m \u001b[39mimport\u001b[39;00m Path\n\u001b[0;32m---> 12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mwavlm\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mWavLM\u001b[39;00m \u001b[39mimport\u001b[39;00m WavLM, WavLMConfig\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mhifigan\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m Generator \u001b[39mas\u001b[39;00m HiFiGAN\n\u001b[1;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mhifigan\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m AttrDict\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wavlm'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from knn_vc import hubconf as hc\n",
    "\n",
    "# model = knn_vc(pretrained=True, progress=True, prematched=True, device='cuda')\n",
    "\n",
    "# model = DummyModel()\n",
    "\n",
    "# print(model.get_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.state_dict())\n",
    "\n",
    "torch.save(model.state_dict(), \"dummymodel_state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSON stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import torchaudio\n",
    "\n",
    "\n",
    "json_file = \"./datasets/debug_set/debugset.json\"\n",
    "with open(json_file, \"r\") as json_file:\n",
    "    json_dict = json.loads(json_file.read()) #loads the config yaml file as a dict\n",
    "\n",
    "print(json_dict)\n",
    "\n",
    "print(os.path.abspath(json_dict[\"source_path\"]))\n",
    "\n",
    "audio = torchaudio.load(json_dict[\"source_path\"])\n",
    "\n",
    "print(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# def generate_json_files_for_inference():\n",
    "dataset_path = \"./datasets/LibriSpeech/test-clean\"\n",
    "num_readers = len(next(os.walk(dataset_path))[1]) # number of folders, each is a unique reader\n",
    "\n",
    "# select a random reader\n",
    "directories = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n",
    "\n",
    "src_reader_id = np.random.randint(0,num_readers) #randomly select a reader id for the source\n",
    "src_reader_dir = directories[src_reader_id] # reader dir for the source utterances\n",
    "\n",
    "target_reader_id = np.random.randint(0,num_readers)\n",
    "\n",
    "while target_reader_id == src_reader_id:\n",
    "    target_reader_id = np.random.randint(0,num_readers) #so src and target aren't the same\n",
    "\n",
    "target_reader_dir = directories[target_reader_id] # reader dir for the target utterances\n",
    "\n",
    "# src_num_chapters_avail = len(next(os.walk(f\"{dataset_path}/{src_reader_dir}\"))[1]) #how many folders the src reader has to select from\n",
    "# target_num_chapters_avail = len(next(os.walk(f\"{dataset_path}/{target_reader_dir}\"))[1]) #how many folders the target reader has to select from\n",
    "\n",
    "src_chapter_dirs = [d for d in os.listdir(f\"{dataset_path}/{src_reader_dir}\") if os.path.isdir(os.path.join(f\"{dataset_path}/{src_reader_dir}\", d))] #the actual names of the chapter dirs for the chosen speaker\n",
    "target_chapter_dirs = [d for d in os.listdir(f\"{dataset_path}/{target_reader_dir}\") if os.path.isdir(os.path.join(f\"{dataset_path}/{target_reader_dir}\", d))] #the actual names of the chapter dirs for the chosen target\n",
    "\n",
    "src_chapter = random.choice(src_chapter_dirs)\n",
    "target_chapter = random.choice(target_chapter_dirs)\n",
    "\n",
    "src_path = f\"{dataset_path}/{src_reader_dir}/{src_chapter}\"\n",
    "target_path = f\"{dataset_path}/{target_reader_dir}/{target_chapter}\"\n",
    "\n",
    "# choosing a random .flac file for the src utterance\n",
    "src_files = [f for f in os.listdir(src_path) if os.path.isfile(os.path.join(src_path, f))]\n",
    "src_flac_files =[f for f in src_files if f.endswith('.flac')]\n",
    "src_file = random.choice(src_flac_files)\n",
    "\n",
    "# choosing a random number of .flac files for the target utterance\n",
    "target_files = [f for f in os.listdir(target_path) if os.path.isfile(os.path.join(target_path, f))]\n",
    "target_flac_files =[f for f in target_files if f.endswith('.flac')]\n",
    "num_files_to_select = np.random.randint(1, len(target_flac_files)-1)\n",
    "print(num_files_to_select)\n",
    "print(len(target_flac_files))\n",
    "\n",
    "target_files = random.sample(target_flac_files,num_files_to_select)\n",
    "\n",
    "src_path_final = f\"{dataset_path}/{src_reader_dir}/{src_chapter}/{src_file}\"\n",
    "target_paths_final = [f\"{dataset_path}/{target_reader_dir}/{target_chapter}/{target_file}\" for target_file in target_files]\n",
    "\n",
    "data = {\n",
    "    \"source_path\": src_path_final,\n",
    "    \"target_paths\": target_paths_final,\n",
    "    \"working_dir\": \"/media/Data/CambAI/TakeHome/CambAI_TakeHome\"\n",
    "}\n",
    "\n",
    "print(data)\n",
    "\n",
    "# print(f\"Choosing {src_reader_dir}/{src_chapter}/{src_file} for src\")\n",
    "# print(f\"Choosing {num_files_to_select} files {target_reader_dir}/{target_chapter}/{target_files} for target\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "docker run --rm -it -p 127.0.0.1:8080:8080 -p 127.0.0.1:8081:8081 -p 127.0.0.1:8082:8082 --name knn_vc -v /media/Data/CambAI/TakeHome/CambAI_TakeHome/deployment/model_store:/home/model-server/model-store -v /media/Data/CambAI/TakeHome/CambAI_TakeHome/knn_vc:/home/model-server/knn_vc -v /media/Data/CambAI/TakeHome/CambAI_TakeHome/dewet_utils:/home/model-server/dewet_utils pytorch/torchserve:latest-gpu\n",
    "\n",
    "<!-- Custom docker image -->\n",
    "sudo docker run --rm -it -p 127.0.0.1:8080:8080 -p 127.0.0.1:8081:8081 -p 127.0.0.1:8082:8082 --name knn_vc -v /media/Data/CambAI/TakeHome/CambAI_TakeHome/deployment/model_store:/home/model-server/model-store -v /media/Data/CambAI/TakeHome/CambAI_TakeHome/knn_vc:/home/model-server/knn_vc -v /media/Data/CambAI/TakeHome/CambAI_TakeHome/dewet_utils:/home/model-server/dewet_utils pytorch/custom_torchserve:latest\n",
    "\n",
    "\n",
    "sudo docker exec -it knn_vc /bin/bash\n",
    "\n",
    "torch-model-archiver --model-name knn_vc --version 1.0 --model-file /home/model-server/knn_vc/matcher.py --serialized-file /home/model-server/knn_vc/state_dicts/knn_vc_state_dict --export-path /home/model-server/model-store --handler /home/model-server/dewet_utils/knn_vc_handler.py -f\n",
    "\n",
    "\n",
    "<!-- Reload Hanlder -->\n",
    "torch-model-archiver --model-name knn_vc --handler /home/model-server/dewet_utils/knn_vc_handler.py --export-path /home/model-server/model-store -f -v 1.0\n",
    "\n",
    "<!-- Build custom dockerfile -->\n",
    "docker build -t pytorch/custom_torchserve:latest -f Dockerfile.txt .\n",
    "\n",
    "<!-- List dcker images -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/home/model-server/knn_vc/state_dicts/knn_vc_state_dict.pth\n",
    "\n",
    "\n",
    "curl -X POST \"localhost:8081/models?model_name=knn_vc&url=/home/model-server/model-store/knn_vc.mar&initial_workers=4\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
